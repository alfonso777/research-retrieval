{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research-retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13ITasuxzBD7CFld_F1UGRoPQD9ALOJiz",
      "authorship_tag": "ABX9TyMqmH6lm4/oDTy8yLtN6s0t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-qkaaIbHrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dependencies\n",
        "#!pip install pybliometrics\n",
        "!pip install pip elsapy\n",
        "#!pip install python-dotenv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-i6L2URLrT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount= True)\n",
        "#!ls \"/content/drive/My Drive\"\n",
        "import os\n",
        "from configparser import ConfigParser\n",
        "config = ConfigParser()\n",
        "config.read('/content/drive/My Drive/confs/scopus.conf')\n",
        "os.environ['SCOPUS_KEY'] = config['scopus']['APIKey']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH05jZqAahDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pybliometrics.scopus import ScopusSearch\n",
        "from pybliometrics.scopus.utils import config\n",
        "config['Authentication']['APIKey'] = '7ee0d918ac44ed6d0cad0e8ddbcf4258'\n",
        "print(config['Authentication']['APIKey'])\n",
        "\n",
        "articles = ScopusSearch('ISSN 0169-023X', download=False)\n",
        "articles.get_results_size()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qliy5onhjya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global results\n",
        "global results_df\n",
        "from elsapy.elsclient import ElsClient\n",
        "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
        "from elsapy.elsdoc import FullDoc, AbsDoc\n",
        "from elsapy.elssearch import ElsSearch\n",
        "import os\n",
        "import json\n",
        "\n",
        "client = ElsClient(os.environ['SCOPUS_KEY'])\n",
        "articles_search = ElsSearch('ISSN(0169-023X) AND PUBYEAR > 2000', 'scopus')\n",
        "articles_search.execute(client, get_all = False)\n",
        "results = articles_search.results\n",
        "results_df  = articles_search.results_df \n",
        "print(len(results))\n",
        "results[0]\n",
        "#content1  = articles_search.results[0]\n",
        "#print(content1)\n",
        "#print(content1['dc:identifier'])\n",
        "#doc = AbsDoc(content1['dc:identifier'])\n",
        "#doc.read(client)\n",
        "#doc.data\n",
        "\n",
        "\n",
        "\n",
        "#import pandas as pd\n",
        "#df = pd.DataFrame(articles_search.results)\n",
        "#df.head()\n",
        "#print(df.columns)\n",
        "#for index, row in df.iterrows():\n",
        "  #print(row)\n",
        "  #doc = AbsDoc(scp_id = row.eid)\n",
        "  #doc.read(client)\n",
        "  #print(row.eid, row['prism:publicationName'],row['prism:doi'], sep=';')\n",
        "  #print(row['dc:title'], doc.data() if doc.read(client) else  'Not Found',sep='||')\n",
        "  \n",
        "  #print(row['dc:title'], doc.data(),sep='||')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-p5hqLmYO_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#l1 = [ l['@href'] for l in results[0]['link'] if l['@ref']=='scopus']\n",
        "\n",
        "pd.set_option('max_colwidth', -1)\n",
        "\n",
        "results_df.head()\n",
        "#results_df[['dc:identifier','dc:creator','dc:title', 'link', 'prism:doi','prism:issn','prism:volume']]\n",
        "#results[1]\n",
        "#results_df.iloc['link']['scopus']\n",
        "\n",
        "#results_df[['link']]\n",
        "#with pd.option_context('display.max_rows', None, 'display.max_columns', -1):\n",
        "\n",
        "#results_df.loc[:,'link']\n",
        "\n",
        "#results_df.iloc[0].head(1)\n",
        "\n",
        "#results_df['abstract_link'] = results_df['link']['scopus']\n",
        "#del results_df['link']\n",
        "\n",
        "\n",
        "#results_df['scopus_link'] = results_df.loc[results_df['link']['@ref'] == 'scopus', '@href']  #results_df[results_df['link']['@ref'] == ''\n",
        "#results_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3gqkf6ZPtsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "a3e34ae4-3160-466a-fdd4-75b8d6041490"
      },
      "source": [
        "import lxml.html\n",
        "import requests\n",
        "\n",
        "def get_abstract_keywords_by_pii(pii):\n",
        "  article_url = 'https://www.sciencedirect.com/science/article/abs/pii/' + pii\n",
        "  query_abstract = \"//article[@role='main']//div[@id='abstracts']\"\n",
        "  query_keywords = \"//article[@role='main']//div[@class='keywords-section']/div\"\n",
        "  header = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0'\n",
        "  }\n",
        "  response = requests.get(article_url, headers=header)\n",
        "  html = lxml.html.fromstring(response.content.decode('utf-8'))\n",
        "  abstract = html.xpath(query_abstract)[0].text_content()\n",
        "  keywords = html.xpath(query_keywords)[0].text_content()\n",
        "  return (abstract, ', '.join([t.text_content() for t in html.xpath(query_keywords)]))\n",
        "\n",
        "result = get_abstract_keywords_by_pii('S0169023X19304380')\n",
        "print(result[0])\n",
        "print(result[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AbstractGraphs, being an expressive data structure, have become increasingly important for modeling real-world applications, such as collaboration, different kinds of transactions, social networks, to name a few. With the advent of social networks and the web, the graph sizes have grown too large to fit in main memory precipitating the need for alternative approaches for an efficient, scalable evaluation of queries on graphs of any size.In this paper, we use the time-tested “divide and conquer” approach by partitioning a graph into desired number of partitions (and possibly with appropriate characteristics) and process queries over those partitions to obtain all or specified number of answers. This entails correctly computing answers that span multiple partitions or even need the same partition more than once. Given a set of partitions, there are a number of approaches using which a query can be evaluated: (i) One Partition At a Time (OPAT) approach, (ii) Traditional use of Multiple Processors (TraditionalMP), and (iii) using the Map/Reduce Multi-Processor approach (MapReduceMP) approach. The first approach, detailed in this paper, has established scalability through independent processing of partitions. The other two approaches address response time in addition to scalability. For the OPAT query evaluation approach, necessary minimal book keeping has been identified and its correctness established in this paper. Query answering on partitioned graphs also requires analyzing partitioning schemes for their impact on query processing and determining the number as well as the sequence in which partitions need to be loaded to reduce the response time for processing queries. We correlate query properties and partition characteristics to reduce query processing time in terms of the resources available.We also identify a set of quantitative metrics and use them for formulating heuristics to determine the order of loading partitions for efficient query processing. For OPAT approach, extensive experiments on large graphs (synthetic and real-world) using different partitioning schemes analyze the proposed heuristics on a variety of query types. The other two approaches are fleshed out, analyzed, and contrasted with the OPAT approach. An existing graph querying system has been extended to evaluate queries on partitioned graphs. Finally all three approaches are compared for their strengths and weaknesses.\n",
            "Graph query processing, Plan generation, Query evaluation on partitioned graphs, Scalability, Map/Reduce\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxt7v9MqGIlQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "72248698-f431-4fe4-d688-c3d00b1c906c"
      },
      "source": [
        "from elsapy.elsclient import ElsClient\n",
        "from elsapy.elsdoc import FullDoc, AbsDoc\n",
        "import json\n",
        "\n",
        "client = ElsClient(os.environ['SCOPUS_KEY'])\n",
        "doc = AbsDoc(scp_id = '85079901526')\n",
        "#doc = AbsDoc(uri = 'https://api.elsevier.com/content/abstract/scopus_id/85079901526')\n",
        "#doc = FullDoc(doi='10.1016/j.datak.2019.101730')\n",
        "\n",
        "\n",
        "if doc.read(els_client=client):\n",
        "   print(doc.data)\n",
        "else:\n",
        "   print(\"Nothing\")\n",
        "print(\"Request status code: %s\" % client.req_status)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'affiliation': [{'affiliation-city': 'Houston', 'affilname': 'University of Houston', 'affiliation-country': 'United States'}, {'affiliation-city': 'Futuroscope', 'affilname': 'ISAE-ENSMA', 'affiliation-country': 'France'}], 'coredata': {'srctype': 'j', 'eid': '2-s2.0-85079901526', 'prism:coverDate': '2020-03-01', 'prism:aggregationType': 'Journal', 'prism:url': 'https://api.elsevier.com/content/abstract/scopus_id/85079901526', 'subtypeDescription': 'Editorial', 'dc:creator': {'author': [{'ce:given-name': 'Carlos', 'preferred-name': {'ce:given-name': 'Carlos', 'ce:initials': 'C.', 'ce:surname': 'Ordonez', 'ce:indexed-name': 'Ordonez C.'}, '@seq': '1', 'ce:initials': 'C.', '@_fa': 'true', 'affiliation': {'@id': '60005837', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60005837'}, 'ce:surname': 'Ordonez', '@auid': '57203051107', 'author-url': 'https://api.elsevier.com/content/author/author_id/57203051107', 'ce:indexed-name': 'Ordonez C.'}]}, 'link': [{'@_fa': 'true', '@rel': 'self', '@href': 'https://api.elsevier.com/content/abstract/scopus_id/85079901526'}, {'@_fa': 'true', '@rel': 'scopus', '@href': 'https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079901526&origin=inward'}, {'@_fa': 'true', '@rel': 'scopus-citedby', '@href': 'https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85079901526&origin=inward'}], 'prism:publicationName': 'Data and Knowledge Engineering', 'source-id': '24437', 'pii': 'S0169023X19304318', 'citedby-count': '0', 'prism:volume': '126', 'subtype': 'ed', 'dc:title': 'Guest Editorial—DaWaK 2018 Special Issue—Trends in Big Data Analytics', 'openaccess': '0', 'openaccessFlag': 'false', 'prism:doi': '10.1016/j.datak.2019.101730', 'prism:issn': '0169023X', 'article-number': '101730', 'dc:identifier': 'SCOPUS_ID:85079901526', 'dc:publisher': 'Elsevier B.V.'}}\n",
            "Request status code: {'status_code': 200, 'status_msg': 'data retrieved'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}